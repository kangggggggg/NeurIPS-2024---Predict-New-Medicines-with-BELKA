{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI7pAuXmvvKl",
        "outputId": "9b33ec7f-8465-45b4-c3d3-4a1ff458e6a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.14)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdkit\n",
            "  Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from kaggle)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec[http]<=2024.5.0,>=2023.1.0 (from datasets)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (10.3.0)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Installing collected packages: xxhash, requests, rdkit, pyarrow-hotfix, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.0\n",
            "    Uninstalling fsspec-2024.6.0:\n",
            "      Successfully uninstalled fsspec-2024.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-hotfix-0.6 rdkit-2023.9.6 requests-2.32.3 xxhash-3.4.1 yarl-1.9.4\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading leash-BELKA.zip to /content\n",
            "100% 4.16G/4.16G [00:31<00:00, 170MB/s]\n",
            "100% 4.16G/4.16G [00:31<00:00, 140MB/s]\n",
            "Archive:  leash-BELKA.zip\n",
            "  inflating: leash-BELKA/sample_submission.csv  \n",
            "  inflating: leash-BELKA/test.csv    \n",
            "  inflating: leash-BELKA/test.parquet  \n",
            "  inflating: leash-BELKA/train.csv   \n",
            "  inflating: leash-BELKA/train.parquet  \n",
            "Dataset URL: https://www.kaggle.com/datasets/ahmedelfazouan/belka-enc-dataset\n",
            "License(s): unknown\n",
            "Downloading belka-enc-dataset.zip to /content\n",
            "100% 2.87G/2.87G [02:10<00:00, 20.2MB/s]\n",
            "100% 2.87G/2.87G [02:10<00:00, 23.6MB/s]\n",
            "Archive:  belka-enc-dataset.zip\n",
            "  inflating: belka-enc-dataset/test_enc.parquet  \n",
            "  inflating: belka-enc-dataset/train_enc.parquet  \n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle datasets rdkit\n",
        "!pip install fastparquet -q\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = ''\n",
        "os.environ['KAGGLE_KEY'] = ''\n",
        "\n",
        "# 원본 데이터 셋\n",
        "!kaggle competitions download -c leash-BELKA\n",
        "!unzip leash-BELKA.zip -d leash-BELKA\n",
        "\n",
        "!kaggle datasets download -d ahmedelfazouan/belka-enc-dataset\n",
        "!unzip belka-enc-dataset.zip -d belka-enc-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SSuB-CCGwZ4x"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import average_precision_score as APS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z7hmwK7yxS7x"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "\n",
        "    PREPROCESS = False\n",
        "    EPOCHS = 20\n",
        "    BATCH_SIZE = 4096\n",
        "    LR = 1e-3\n",
        "    WD = 0.05\n",
        "\n",
        "    NBR_FOLDS = 15\n",
        "    SELECTED_FOLDS = [0]\n",
        "\n",
        "    SEED = 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I9nkgp0_xTzP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "def set_seeds(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seeds(seed=CFG.SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c42C4tbKxYNz"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# # Detect hardware, return appropriate distribution strategy\n",
        "# try:\n",
        "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu=\"local\") # \"local\" for 1VM TPU\n",
        "#     strategy = tf.distribute.TPUStrategy(tpu)\n",
        "#     print(\"Running on TPU\")\n",
        "#     print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "# except tf.errors.NotFoundError:\n",
        "#     print(\"Not on TPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN5SNGgN2-XY",
        "outputId": "78210f02-58ea-430e-e455-1dcbabca2ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on TPU\n",
            "REPLICAS:  8\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\") # \"local\" for 1VM TPU\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print(\"Running on TPU\")\n",
        "    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
        "except tf.errors.NotFoundError:\n",
        "    print(\"Not on TPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y31yHA3_xcvo"
      },
      "outputs": [],
      "source": [
        "if CFG.PREPROCESS:\n",
        "    enc = {'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n",
        "           '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25, '=': 26,\n",
        "           '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36}\n",
        "    train_raw = pd.read_parquet('/content/leash-BELKA/train.parquet')\n",
        "    smiles = train_raw[train_raw['protein_name']=='BRD4']['molecule_smiles'].values\n",
        "    assert (smiles!=train_raw[train_raw['protein_name']=='HSA']['molecule_smiles'].values).sum() == 0\n",
        "    assert (smiles!=train_raw[train_raw['protein_name']=='sEH']['molecule_smiles'].values).sum() == 0\n",
        "    def encode_smile(smile):\n",
        "        tmp = [enc[i] for i in smile]\n",
        "        tmp = tmp + [0]*(142-len(tmp))\n",
        "        return np.array(tmp).astype(np.uint8)\n",
        "\n",
        "    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n",
        "    smiles_enc = np.stack(smiles_enc)\n",
        "    train = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n",
        "    train['bind1'] = train_raw[train_raw['protein_name']=='BRD4']['binds'].values\n",
        "    train['bind2'] = train_raw[train_raw['protein_name']=='HSA']['binds'].values\n",
        "    train['bind3'] = train_raw[train_raw['protein_name']=='sEH']['binds'].values\n",
        "    train.to_parquet('train_enc.parquet')\n",
        "\n",
        "    test_raw = pd.read_parquet('/content/leash-BELKA/test.parquet')\n",
        "    smiles = test_raw['molecule_smiles'].values\n",
        "\n",
        "    smiles_enc = joblib.Parallel(n_jobs=96)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n",
        "    smiles_enc = np.stack(smiles_enc)\n",
        "    test = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n",
        "    test.to_parquet('test_enc.parquet')\n",
        "\n",
        "else:\n",
        "    train = pd.read_parquet('/content/belka-enc-dataset/train_enc.parquet')\n",
        "    test = pd.read_parquet('/content/belka-enc-dataset/test_enc.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_6vO-7K4xfRX"
      },
      "outputs": [],
      "source": [
        "# LSTM\n",
        "\n",
        "def my_model():\n",
        "    with strategy.scope():\n",
        "        INP_LEN = 142\n",
        "        hidden_dim = 128\n",
        "\n",
        "        inputs = tf.keras.layers.Input(shape=(INP_LEN,), dtype='int32')\n",
        "        x = tf.keras.layers.Embedding(input_dim=36, output_dim=hidden_dim, input_length=INP_LEN, mask_zero=True)(inputs)\n",
        "        x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
        "        x = tf.keras.layers.LSTM(128)(x)\n",
        "\n",
        "        x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "        x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "        x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "        x = tf.keras.layers.Dropout(0.1)(x)\n",
        "\n",
        "        outputs = tf.keras.layers.Dense(3, activation='sigmoid')(x)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=CFG.LR, weight_decay=CFG.WD)\n",
        "        loss = 'binary_crossentropy'\n",
        "        weighted_metrics = [tf.keras.metrics.AUC(curve='PR', name='avg_precision')]\n",
        "        model.compile(\n",
        "            loss=loss,\n",
        "            optimizer=optimizer,\n",
        "            weighted_metrics=weighted_metrics,\n",
        "        )\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNESh6K7xhN0",
        "outputId": "964acf2e-1bdb-4ddd-e5f5-6c2fc15dff08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "18962/22426 [========================>.....] - ETA: 14:25 - loss: 0.0176 - avg_precision: 0.4498"
          ]
        }
      ],
      "source": [
        "FEATURES = [f'enc{i}' for i in range(142)]\n",
        "TARGETS = ['bind1', 'bind2', 'bind3']\n",
        "skf = StratifiedKFold(n_splits = CFG.NBR_FOLDS, shuffle = True, random_state = 42)\n",
        "\n",
        "all_preds = []\n",
        "for fold,(train_idx, valid_idx) in enumerate(skf.split(train, train[TARGETS].sum(1))):\n",
        "\n",
        "    if fold not in CFG.SELECTED_FOLDS:\n",
        "        continue;\n",
        "\n",
        "    X_train = train.loc[train_idx, FEATURES]\n",
        "    y_train = train.loc[train_idx, TARGETS]\n",
        "    X_val = train.loc[valid_idx, FEATURES]\n",
        "    y_val = train.loc[valid_idx, TARGETS]\n",
        "\n",
        "    es = tf.keras.callbacks.EarlyStopping(patience=5, monitor=\"val_loss\", mode='min', verbose=1)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(monitor='val_loss', filepath=f\"model-{fold}.h5\",\n",
        "                                                        save_best_only=True, save_weights_only=True,\n",
        "                                                    mode='min')\n",
        "    reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, verbose=1)\n",
        "    model = my_model()\n",
        "    history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=CFG.EPOCHS,\n",
        "            callbacks=[checkpoint, reduce_lr_loss, es],\n",
        "            batch_size=CFG.BATCH_SIZE,\n",
        "            verbose=1,\n",
        "        )\n",
        "    model.load_weights(f\"model-{fold}.h5\")\n",
        "    oof = model.predict(X_val, batch_size = 2*CFG.BATCH_SIZE)\n",
        "    print('fold :', fold, 'CV score =', APS(y_val, oof, average = 'micro'))\n",
        "\n",
        "    preds = model.predict(test, batch_size = 2*CFG.BATCH_SIZE)\n",
        "    all_preds.append(preds)\n",
        "\n",
        "preds = np.mean(all_preds, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEP4isXWxjhL"
      },
      "outputs": [],
      "source": [
        "tst = pd.read_parquet('/content/leash-BELKA/test.parquet')\n",
        "tst['binds'] = 0\n",
        "tst.loc[tst['protein_name']=='BRD4', 'binds'] = preds[(tst['protein_name']=='BRD4').values, 0]\n",
        "tst.loc[tst['protein_name']=='HSA', 'binds'] = preds[(tst['protein_name']=='HSA').values, 1]\n",
        "tst.loc[tst['protein_name']=='sEH', 'binds'] = preds[(tst['protein_name']=='sEH').values, 2]\n",
        "tst[['id', 'binds']].to_csv('submission.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9tmOmIk3KOw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
